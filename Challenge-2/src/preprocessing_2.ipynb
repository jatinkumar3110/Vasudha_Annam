{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b4456",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError, Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import seaborn as snsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dc82e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Mount Google Drive or upload data via Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfc137",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==== CONFIG ==== #\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_DIR = Path('/content/drive/MyDrive/soil-classification-part-2/soil_competition-2025/train')\n",
    "TEST_DIR = Path('/content/drive/MyDrive/soil-classification-part-2/soil_competition-2025/test')\n",
    "\n",
    "train_csv = '/content/drive/MyDrive/soil-classification-part-2/soil_competition-2025/train_labels.csv'\n",
    "test_csv = '/content/drive/MyDrive/soil-classification-part-2/soil_competition-2025/test_ids.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56909d7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#List of files in TEST_DIR and to extarct Unique file extensions\n",
    "test_files = os.listdir(TEST_DIR)\n",
    "\n",
    "extensions = [os.path.splitext(f)[1].lower().replace(\".\", \"\") for f in test_files if os.path.isfile(os.path.join(TEST_DIR, f))]\n",
    "\n",
    "# Count and display unique types\n",
    "ext_counts = Counter(extensions)\n",
    "print(\" Unique image file types in test set:\")\n",
    "for ext, count in ext_counts.items():\n",
    "    print(f\" {ext}: {count} file(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd01363",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of files in TRAIN_DIR to extract Unique file extensions\n",
    "train_files = os.listdir(TRAIN_DIR)\n",
    "\n",
    "extensions = [os.path.splitext(f)[1].lower().replace(\".\", \"\") for f in train_files if os.path.isfile(os.path.join(TRAIN_DIR, f))]\n",
    "\n",
    "# Count unique types\n",
    "ext_counts = Counter(extensions)\n",
    "print(\" Unique image file types in training set:\")\n",
    "for ext, count in ext_counts.items():\n",
    "    print(f\"{ext}: {count} file(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fe111",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Extratcing file name of special types (png, webp and gif for analysis)\n",
    "import os\n",
    "\n",
    "def list_specific_file_types(directory, extensions_to_find):\n",
    "    files = os.listdir(directory)\n",
    "    filtered_files = [f for f in files if os.path.isfile(os.path.join(directory, f))\n",
    "                      and os.path.splitext(f)[1].lower().replace('.', '') in extensions_to_find]\n",
    "    return filtered_files\n",
    "\n",
    "target_extensions = {'png', 'webp', 'gif'}\n",
    "\n",
    "train_specific_files = list_specific_file_types(TRAIN_DIR, target_extensions)\n",
    "test_specific_files = list_specific_file_types(TEST_DIR, target_extensions)\n",
    "\n",
    "# Display results\n",
    "print(\" Train images with png, webp, or gif extensions:\")\n",
    "for fname in train_specific_files:\n",
    "    print(\"‚Ä¢\", fname)\n",
    "\n",
    "print(\"\\n Test images with png, webp, or gif extensions:\")\n",
    "for fname in test_specific_files:\n",
    "    print(\"‚Ä¢\", fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c1a62",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Image resolution stats\n",
    "def get_image_dims(path_list):\n",
    "    dims = []\n",
    "    for file in path_list:\n",
    "        try:\n",
    "            with Image.open(file) as img:\n",
    "                dims.append(img.size)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(dims, columns=[\"width\", \"height\"])\n",
    "\n",
    "train_files = list(TRAIN_DIR.glob(\"*\"))\n",
    "test_files = list(TEST_DIR.glob(\"*\"))\n",
    "\n",
    "train_dims_df = get_image_dims(train_files)\n",
    "test_dims_df = get_image_dims(test_files)\n",
    "\n",
    "# Outputs for visualization\n",
    "(train_dims_df.describe(), test_dims_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45509ecd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Function to visulaize dataset (train/test)\n",
    "\n",
    "def analyze_image_folder(folder_path, dataset_name=\"Dataset\"):\n",
    "    formats_count = defaultdict(int)\n",
    "    dimensions = []\n",
    "    file_sizes = []\n",
    "    corrupt_files = []\n",
    "\n",
    "    folder_path = Path(folder_path)\n",
    "\n",
    "    # üß™ Analyze each image\n",
    "    for img_path in folder_path.iterdir():\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                formats_count[img.format.lower()] += 1\n",
    "                dimensions.append(img.size)\n",
    "                file_sizes.append(os.path.getsize(img_path))\n",
    "        except UnidentifiedImageError:\n",
    "            corrupt_files.append(img_path.name)\n",
    "\n",
    "    # Skip analysis if no images were read\n",
    "    if not dimensions:\n",
    "        print(f\"‚ùå No valid images found in {dataset_name}\")\n",
    "        return\n",
    "\n",
    "    # üìà Convert to arrays\n",
    "    widths, heights = zip(*dimensions)\n",
    "    aspect_ratios = np.array(widths) / np.array(heights)\n",
    "\n",
    "    print(f\"\\nüìÇ {dataset_name} Analysis\")\n",
    "    print(f\"‚ö†Ô∏è Corrupt/Unreadable Files: {len(corrupt_files)}\")\n",
    "    if corrupt_files:\n",
    "        print(\"Corrupt file names:\", corrupt_files)\n",
    "\n",
    "\n",
    "    print(f\"‚Ä¢ Aspect Ratio: min={aspect_ratios.min():.2f}, max={aspect_ratios.max():.2f}\")\n",
    "    print(f\"‚Ä¢ File Size (KB): min={np.min(file_sizes)/1024:.1f}, max={np.max(file_sizes)/1024:.1f}, mean={np.mean(file_sizes)/1024:.1f}\")\n",
    "\n",
    "    return widths, heights, aspect_ratios, file_sizes\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
